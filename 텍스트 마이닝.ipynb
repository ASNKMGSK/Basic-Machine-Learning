{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16장 텍스트 마이닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 웹 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 모듈 및 함수 불러오기\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "#2. 리뷰와 평점을 담을 빈 리스트 생성\n",
    "score = []\n",
    "review = []\n",
    "\n",
    "#3. 리뷰 스크래핑\n",
    "for i in range(1, 1001) :\n",
    "    #3-1. HTML 소스코드 가져오기\n",
    "    base_url=\"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=161967&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=\"+str(i)\n",
    "    html = requests.get(base_url)\n",
    "\n",
    "    #3-2. HTML 파싱\n",
    "    soup = bs(html.text, 'html.parser')\n",
    "\n",
    "    #3-3. 평점 및 리뷰 텍스트 추출\n",
    "    for j in range(10):\n",
    "        score.append(soup.select('div.star_score > em')[j].text)\n",
    "        review.append(soup.find_all('span', {'id': re.compile('_filtered_ment_\\d')})[j].text.strip())\n",
    "        \n",
    "#4.하나의 데이터프레임으로 저장\n",
    "df = pd.DataFrame(list(zip(review, score)), columns=['review', 'score'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 빈도 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 모듈 및 함수 불러오기\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "#2. 문자열 변환\n",
    "string= \",\".join(review)\n",
    "\n",
    "#3. 명사 추출\n",
    "okt = Okt( )\n",
    "nouns=okt.nouns(string)\n",
    "\n",
    "#4. 두 글자 이상 단어 추출\n",
    "word_list =[x for x in nouns if len(x) >= 2]\n",
    "\n",
    "#5. 불용어 제거\n",
    "stopwords=[\"영화\", \"그냥\", \"정말\", \"진짜\"]\n",
    "word_list=[i for i in word_list if i not in stopwords]\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빈도 분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(word_list)\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 워드클라우드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1. 라이브러리 설치 및 모듈/함수 불러오기\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#2. 워드클라우드 생성\n",
    "font_path = '/usr/share/fonts/truetype/nanum/malgun.ttf'\n",
    "wc = WordCloud(width = 800, height = 800, font_path = font_path, background_color='white')\n",
    "\n",
    "#3. 시각화\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wc.generate_from_frequencies(count))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 버즈 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오픈 API를 호출하여 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 라이브러리 불러오기\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "#2. 요청 정보 입력\n",
    "request = urllib.request.Request(\"https://openapi.naver.com/v1/datalab/search\")\n",
    "request.add_header(\"X-Naver-Client-Id\", \"vppPR7VLu9XbWzzbCuDL\")\n",
    "request.add_header(\"X-Naver-Client-Secret\",\"O758KnQqQm\")\n",
    "request.add_header(\"Content-Type\",\"application/json\") \n",
    "\n",
    "#3. 요청 본문 생성\n",
    "body_dict = {\"startDate\": \"2019-01-01\",\n",
    "            \"endDate\" : \"2019-12-20\",\n",
    "            \"timeUnit\": \"month\",\n",
    "            \"keywordGroups\":[{\"groupName\": \"기생충\",\"keywords\":[\"기생충\"]}]}\n",
    "body = json.dumps(body_dict)\n",
    "body \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 서버에 정보 요청\n",
    "response = urllib.request.urlopen(request, data=body.encode(\"utf-8\"))\n",
    "\n",
    "#2. 응답 상태 코드 가져오기\n",
    "rescode = response.getcode( )\n",
    "\n",
    "#3. 값이 200인 경우에만 데이터 추출\n",
    "if(rescode==200):\n",
    "    scraped = response.read( )\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)\n",
    "\n",
    "#4. json 데이터 타입 변환\n",
    "result = json.loads(scraped)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 버즈 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 모듈 및 함수 임포트\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#2. 데이터 프레임 변환\n",
    "data = pd.DataFrame(result[\"results\"][0][\"data\"]).set_index('period')\n",
    "\n",
    "#3. 그래프 생성\n",
    "data.plot(color = 'g', figsize = (20,10))\n",
    "plt.legend(fontsize=30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 토픽 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 모듈 및 함수 불러오기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#2. 문서 단어 행렬 변환\n",
    "stopword = ['영화', '하지만', '근데', '대한', '이게', '없는', '영화가', '영화는', '영화를', 'ㅋㅋ', '내내', '봤습니다', '보고', '보는', '그냥', '많이', '내가',  '그리고', '진짜', '정말', '너무', '나는', '있는', '가장', 'ㅎㅎ']\n",
    "tv= TfidfVectorizer(max_df=.15, ngram_range=(1,4), min_df=2, stop_words = stopword)\n",
    "vect = tv.fit_transform(review)\n",
    "\n",
    "print(\"문서 단어 행렬 변환 결과: \\n\", vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토픽 모델링 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 모듈 및 함수 불러오기\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#2. LDA 모형 생성 및 변환\n",
    "model = LatentDirichletAllocation(n_components=5, learning_method=\"batch\", max_iter=25, random_state=0)\n",
    "model.fit_transform(vect)\n",
    "\n",
    "#3. 토픽별 주요 단어 출력\n",
    "for topic_index, topic in enumerate(model.components_):\n",
    "    print('Topic #',topic_index+1) \n",
    "    topic_index = topic.argsort()[::-1]\n",
    "    feature_names = ' '.join([tv.get_feature_names()[i] for i in topic_index[:10]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
